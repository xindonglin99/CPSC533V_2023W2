{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "authors: Matthew Wilson, Daniele Reda\n",
    "created: 2020/01/14\n",
    "last_updated: 2023/02/08\n",
    "-->\n",
    "\n",
    "\n",
    "## CPSC 533V: Assignment 3 - Tabular Q Learning and DQN (Due Thu Feb 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#  Part 1 [54 pts] Tabular Q-Learning \n",
    "\n",
    "Tabular Q-learning is an RL algorithm for problems with discrete states and discrete actions. The algorithm is described in the class notes, which borrows the summary description from [Section 6.5](http://incompleteideas.net/book/RLbook2018.pdf#page=153) of Richard Sutton's RL book. In the tabular approach, the Q-value is represented as a lookup table. As discussed in class, Q-learning can further be extended to continuous states and discrete actions, leading to the [Atari DQN](https://arxiv.org/abs/1312.5602) / Deep Q-learning algorithm.  However, it is important and informative to first fully understand tabular Q-learning.\n",
    "\n",
    "Informally, Q-learning works as follows: The goal is to learn the optimal Q-function: \n",
    "`Q(s,a)`, which is the *value* of being at state `s` and taking action `a`.  Q tells you how well you expect to do, on average, from here on out, given that you act optimally.  Once the Q function is learned, choosing an optimal action is as simple as looping over all possible actions and choosing the one with the highest Q (optimal action $a^* = \\text{argmax}_a Q(s,a)$).  To learn Q, we initialize it arbitrarily and then iteratively refine it using the Bellman backup equation for Q functions, namely: \n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\text{max}_a Q(s', a) - Q(s,a)]$.\n",
    "Here, $r$ is the reward associated with with the transition from state s to s', and $\\alpha$ is a learning rate.\n",
    "\n",
    "In the first part of assignment you will implement tabular Q-learning and apply it to CartPole -- an environment with a **continuous** state space.  To apply the tabular method, you will need to discretize the CartPole state space by dividing the state-space into bins.\n",
    "\n",
    "\n",
    "**Goals:**\n",
    "- to become familiar with python/numpy, as well as using an OpenAI Gym environment\n",
    "- to understand tabular Q-learning, by implementing tabular Q-Learning for \n",
    "  a discretized version of a continuous-state environment, and experimenting with the implementation\n",
    "- (optional) to develop further intuition regarding possible variations of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Deep reinforcement learning has generated impressive results for board games ([Go][go], [Chess/Shogi][chess]), video games ([Atari][atari], [DOTA2][dota], [StarCraft II][scii]), [and][baoding] [robotic][rubix] [control][anymal] ([of][cassie] [course][mimic] ;)).  RL is beginning to work for an increasing range of tasks and capabilities.  At the same time, there are many [gaping holes][irpan] and [difficulties][amid] in applying these methods. Understanding deep RL is important if you wish to have a good grasp of the modern landscape of control methods.\n",
    "\n",
    "These next several assignments are designed to get you started with deep reinforcement learning, to give you a more close and personal understanding of the methods, and to provide you with a good starting point from which you can branch out into topics of interest. You will implement basic versions of some of the important fundamental algorithms in this space, including Q-learning and policy gradient/search methods.\n",
    "\n",
    "We will only have time to cover a subset of methods and ideas in this space.\n",
    "If you want to dig deeper, we suggest following the links given on the course webpage.  Additionally we draw special attention to the [Sutton book](http://incompleteideas.net/book/RLbook2018.pdf) for RL fundamentals and in depth coverage, and OpenAI's [Spinning Up resources](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) for a concise intro to RL and deep RL concepts, as well as good comparisons and implementations of modern deep RL algorithms.\n",
    "\n",
    "\n",
    "[atari]: https://arxiv.org/abs/1312.5602\n",
    "[go]: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[chess]:https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go \n",
    "[dota]: https://openai.com/blog/openai-five/\n",
    "[scii]: https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
    "[baoding]: https://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/\n",
    "[rubix]: https://openai.com/blog/solving-rubiks-cube/\n",
    "[cassie]: https://www.cs.ubc.ca/~van/papers/2019-CORL-cassie/index.html\n",
    "[mimic]: https://www.cs.ubc.ca/~van/papers/2018-TOG-deepMimic/index.html\n",
    "[anymal]: https://arxiv.org/abs/1901.08652\n",
    "\n",
    "\n",
    "[irpan]: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "[amid]: http://amid.fish/reproducing-deep-rl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment if necesary\n",
    "# !pip install numpy\n",
    "# !pip install gym\n",
    "# # OR:\n",
    "# !pip install gymnasium\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [12 pts] Explore the CartPole environment \n",
    "\n",
    "Your first task is to familiarize yourself with the OpenAI gym interface and the [CartPole environment](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)\n",
    "by writing a simple hand-coded policy to try to solve it.  \n",
    "To begin understanding OpenAI Gym environments, [read this first](https://gymnasium.farama.org/api/env/).) \n",
    "The gym interface is very popular and you will see many algorithm implementations and \n",
    "custom environments that support it.  You may even want to use the API in your course projects, \n",
    "to define a custom environment for a task you want to solve.\n",
    "\n",
    "Note that there were several breaking changes introduced in the past few years to the gym API. Some reference algorithm implementations online might be using the old version:\n",
    "- `obs = env.reset()` ->  `obs, info = env.reset()`\n",
    "- `obs, reward, done, info = env.step(action)` to `obs, reward, terminated, truncated, info = env.step(action)`\n",
    "- `env.render()` no longer accepts the `render_mode` parameter (e.g. human mode where the environment is rendered in a pop-up window, or rgb_array which allows headless conversion to images or videos)\n",
    "\n",
    "\n",
    "Below is some example code that runs a simple random policy.  You are to:\n",
    "- **run the code to see what it does**\n",
    "- **write code that chooses an action based on the observation**.  You will need to learn about the gym API and to read the CartPole documentation to figure out what the `action` and `obs` vectors mean for this environment. \n",
    "Your hand-coded policy can be arbitrary, and it should ideally do better than the random policy.  There is no single correct answer. The goal is to become familiar with `env`s.\n",
    "- **write code to print out the total reward gained by your policy in a single episode run**\n",
    "- **answer the short-response questions below** (see the TODOs for all of this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")  # you can also try LunarLander-v2, but make sure to change it back\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# To find out what the observations mean, read the CartPole documentation.\n",
    "# Uncomment the lines below, or visit the source file: \n",
    "# https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py\n",
    "\n",
    "#cartpole = env.unwrapped\n",
    "#cartpole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.0\n"
     ]
    }
   ],
   "source": [
    "# 1.1 [10pts]\n",
    "\n",
    "# runs a single episode and render it.  try running this before editing anything\n",
    "obs, info = env.reset()  # get first obs/state\n",
    "rewards = 0\n",
    "while True:\n",
    "    # TODO: replace this `action` with something that depends on `obs` \n",
    "    if obs[2] > 0:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "    # action = env.action_space.sample()  # random action\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards += reward\n",
    "    env.render()\n",
    "    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "    if terminated | truncated: break\n",
    "env.close()\n",
    "\n",
    "# TODO: print out your total sum of rewards here\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the questions below, look at the full [source code here](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py) if you haven't already.\n",
    "\n",
    "**1.2. [2pts] Briefly describe your policy.  What observation information does it use?  What score did you achieve (rough maximum and average)?  And how does it compare to the random policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the pole angle. If pole angle > 0, means pole leaning right, so push right to let it maintain balance. Vice versa. Maximum reward is 53, average 44. It's higher than the random policy (reward 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  [12 pts] Discretize the env\n",
    "\n",
    "Next, we need to discretize CartPole's continuous state space to work for tabular Q-learning.  While this is in part  a contrived usage of tabular methods, given the existence of other approaches that are designed to cope with continuous state-spaces, it is also interesting to consider whether tabular methods can be adapted more directly via discretization of the state into bins. Furthermore, tabular methods are simple, interpretabile, and can be proved to converge, and thus they still remain relevant.\n",
    "\n",
    "Your task is to discretize the state/observation space so that it is compatible with tabular Q-learning.  To do this:\n",
    "- **implement `obs_normalizer` to pass its test**\n",
    "- **implement `get_bins` to pass its test**\n",
    "- **then answer question 2.3**\n",
    "\n",
    "[map]: https://arxiv.org/abs/1504.04909\n",
    "[qd]: https://quality-diversity.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 [5 pts for passing test_normed]\n",
    "def obs_normalizer(obs):\n",
    "    \"\"\"Normalize the observations between 0 and 1\n",
    "    \n",
    "    If the observation has extremely large bounds, then clip to a reasonable range before normalizing; \n",
    "    (-2,2) should work.  (It is ok if the solution is specific to CartPole)\n",
    "    \n",
    "    Args:\n",
    "        obs (np.ndarray): shape (4,) containing an observation from CartPole using the bound of the env\n",
    "    Returns:\n",
    "        normed (np.ndarray): shape (4,) where all elements are roughly uniformly mapped to the range [0, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # HINT: check out env.observation_space.high, env.observation_space.low\n",
    "    \n",
    "    # TODO: implement this function\n",
    "    result = np.zeros_like(obs)\n",
    "    result[0] = (obs[0] / (2*4.8000002)) + 0.5\n",
    "    if obs[1] > 2:\n",
    "        result[1] = 2\n",
    "    elif obs[1] < -2:\n",
    "        result[1] = -2\n",
    "    else:\n",
    "        result[1] = obs[1]\n",
    "    result[1] = result[1] / 4 + 0.5\n",
    "    result[2] = (obs[1] / (2*4.1887903)) + 0.5\n",
    "    if obs[3] > 2:\n",
    "        result[3] = 2\n",
    "    elif obs[3] < -2:\n",
    "        result[3] = -2\n",
    "    else:\n",
    "        result[3] = obs[3]\n",
    "    result[3] = result[3] / 4 + 0.5\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.1\n",
    "def test_normed():\n",
    "    obs, info = env.reset()\n",
    "    while True:\n",
    "        obs, _, terminated, truncated, _ =  env.step(env.action_space.sample())\n",
    "        normed = obs_normalizer(obs) \n",
    "        assert np.all(normed >= 0.0) and np.all(normed <= 1.0), '{} are outside of (0,1)'.format(normed)\n",
    "        if terminated | truncated: break\n",
    "    env.close()\n",
    "    print('Passed!')\n",
    "test_normed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 [5 pts for passing test_binned]\n",
    "def get_bins(normed, num_bins):\n",
    "    \"\"\"Map normalized observations (0,1) to bin index values (0,num_bins-1)\n",
    "    \n",
    "    Args:\n",
    "        normed (np.ndarray): shape (4,) output from obs_normalizer\n",
    "        num_bins (int): how many bins to use\n",
    "    Returns:\n",
    "        binned (np.ndarray of type np.int32): shape (4,) where all elements are values in range [0,num_bins-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    bin_size = 1 / num_bins\n",
    "\n",
    "    # handle corner case\n",
    "    normed[normed == 1] -= (0.1 * bin_size)\n",
    "    return (normed // bin_size).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.2\n",
    "obs, info = env.reset()\n",
    "\n",
    "def test_binned(num_bins):\n",
    "    normed = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "    binned = get_bins(normed, num_bins)\n",
    "    assert np.all(binned >= 0) and np.all(binned < num_bins), '{} supposed to be between (0, {})'.format(binned, num_bins-1)\n",
    "    assert binned.dtype == np.int32, \"You should also make sure to cast your answer to int using arr.astype(np.int32)\" \n",
    "    \n",
    "test_binned(5)\n",
    "test_binned(10)\n",
    "test_binned(50)\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3. [2 pts] If your state has 4 values and each is binned into N possible bins, how many bins are needed to represent all unique possible states?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N<sup>4</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [20 pts] Solve the env \n",
    "\n",
    "Using the pseudocode below and the functions you implemented above, implement tabular Q-learning and use it to solve CartPole.\n",
    "\n",
    "We provide setup code to initialize the Q-table and give examples of interfacing with it. Write the inner and outer loops to train your algorithm.  These training loops will be similar to those deep RL approaches, so get used to writing them!\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![Sutton RL](https://i.imgur.com/mdcWVRL.png)\n",
    "\n",
    "in summary:\n",
    "- **implement Q-learning using this pseudocode and the helper code**\n",
    "- **answer the questions below**\n",
    "- **run the suggested experiments and otherwise experiment with whatever interests you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q Table:  (30, 30, 30, 30, 2)\n",
      "Original obs [-0.025777    0.00829503  0.02939578 -0.02648045] --> binned (14, 15, 15, 14)\n",
      "Value of Q Table at that obs/state value [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# setup (see last few lines for how to use the Q-table)\n",
    "\n",
    "# hyper parameters. feel free to change these as desired and experiment with different values\n",
    "num_bins = 30\n",
    "alpha = 0.5\n",
    "gamma = 0.99\n",
    "log_n = 1000\n",
    "# epsilon greedy\n",
    "eps = 0.05  #usage: action = optimal if np.random.rand() > eps else random\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Q-table initialized to zeros.  first 4 dims are state, last dim is for action (0,1) for left,right.\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "\n",
    "# helper function to convert observation into a binned state so we can index into our Q-table\n",
    "obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "\n",
    "s = obs2bin(obs)\n",
    "\n",
    "print('Shape of Q Table: ', Q.shape) # you can imagine why tabular learning does not scale very well\n",
    "print('Original obs {} --> binned {}'.format(obs, s))\n",
    "print('Value of Q Table at that obs/state value', Q[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current reward is 41.445.\n",
      "The current reward is 57.4995.\n",
      "The current reward is 68.727.\n",
      "The current reward is 72.3615.\n",
      "The current reward is 76.0974.\n",
      "The current reward is 78.33683333333333.\n",
      "The current reward is 81.145.\n",
      "The current reward is 84.83275.\n",
      "The current reward is 86.13066666666667.\n",
      "The current reward is 87.7766.\n",
      "The current reward is 90.81.\n",
      "The current reward is 91.914.\n",
      "The current reward is 93.97061538461539.\n",
      "The current reward is 96.88914285714286.\n",
      "The current reward is 99.11893333333333.\n",
      "The current reward is 100.1260625.\n",
      "The current reward is 101.66235294117647.\n",
      "The current reward is 103.5195.\n",
      "The current reward is 104.56005263157894.\n",
      "The current reward is 105.53015.\n",
      "The current reward is 106.46419047619048.\n",
      "The current reward is 107.71972727272727.\n",
      "The current reward is 108.79086956521739.\n",
      "The current reward is 109.67795833333334.\n",
      "The current reward is 110.04208.\n",
      "The current reward is 111.09026923076924.\n",
      "The current reward is 111.77588888888889.\n",
      "The current reward is 112.93464285714286.\n",
      "The current reward is 113.59693103448276.\n",
      "The current reward is 113.8646.\n",
      "The current reward is 114.54232258064516.\n",
      "The current reward is 115.01075.\n",
      "The current reward is 116.05984848484849.\n",
      "The current reward is 116.83270588235294.\n",
      "The current reward is 117.19928571428571.\n",
      "The current reward is 117.43286111111111.\n",
      "The current reward is 117.9511891891892.\n",
      "The current reward is 118.36892105263158.\n",
      "The current reward is 118.76953846153846.\n",
      "The current reward is 119.29535.\n",
      "The current reward is 120.34236585365854.\n",
      "The current reward is 120.99442857142857.\n",
      "The current reward is 121.5637441860465.\n",
      "The current reward is 121.93279545454546.\n",
      "The current reward is 122.48035555555556.\n",
      "The current reward is 123.16578260869565.\n",
      "The current reward is 123.45148936170213.\n",
      "The current reward is 123.74620833333333.\n",
      "The current reward is 124.17126530612245.\n",
      "The current reward is 124.39128.\n",
      "The current reward is 124.53645098039216.\n",
      "The current reward is 124.73273076923077.\n",
      "The current reward is 125.21126415094339.\n",
      "The current reward is 125.36866666666667.\n",
      "The current reward is 125.69838181818182.\n",
      "The current reward is 125.983125.\n",
      "The current reward is 126.3641403508772.\n",
      "The current reward is 126.3721551724138.\n",
      "The current reward is 126.86603389830509.\n",
      "The current reward is 127.4426.\n",
      "The current reward is 127.89339344262295.\n",
      "The current reward is 128.12440322580645.\n",
      "The current reward is 128.43090476190477.\n",
      "The current reward is 128.59440625.\n",
      "The current reward is 128.87013846153846.\n",
      "The current reward is 129.0405606060606.\n",
      "The current reward is 129.21282089552238.\n",
      "The current reward is 129.52489705882354.\n",
      "The current reward is 129.77639130434784.\n",
      "The current reward is 130.10892857142858.\n",
      "The current reward is 130.40167605633803.\n",
      "The current reward is 130.59195833333334.\n",
      "The current reward is 130.60490410958903.\n",
      "The current reward is 131.07432432432432.\n",
      "The current reward is 131.49882666666667.\n",
      "The current reward is 131.70167105263158.\n",
      "The current reward is 131.91832467532467.\n",
      "The current reward is 132.05989743589743.\n",
      "The current reward is 132.24711392405064.\n",
      "The current reward is 132.5621.\n",
      "The current reward is 132.9579012345679.\n",
      "The current reward is 133.0670731707317.\n",
      "The current reward is 133.24215662650602.\n",
      "The current reward is 133.5860476190476.\n",
      "The current reward is 133.65303529411764.\n",
      "The current reward is 133.74109302325581.\n",
      "The current reward is 134.03603448275862.\n",
      "The current reward is 134.0748977272727.\n",
      "The current reward is 134.22044943820225.\n",
      "The current reward is 134.43422222222222.\n",
      "The current reward is 134.67296703296702.\n",
      "The current reward is 134.8459239130435.\n",
      "The current reward is 135.01751612903226.\n",
      "The current reward is 135.19768085106384.\n",
      "The current reward is 135.51354736842106.\n",
      "The current reward is 135.73379166666666.\n",
      "The current reward is 135.83641237113403.\n",
      "The current reward is 135.90392857142857.\n",
      "The current reward is 135.91528282828284.\n",
      "The current reward is 136.09788.\n",
      "The current reward is 136.30878217821783.\n",
      "The current reward is 136.57038235294118.\n",
      "The current reward is 136.99491262135922.\n",
      "The current reward is 137.16327884615384.\n",
      "The current reward is 137.42875238095237.\n",
      "The current reward is 137.6960566037736.\n",
      "The current reward is 137.96985046728972.\n",
      "The current reward is 138.12155555555555.\n",
      "The current reward is 138.22253211009175.\n",
      "The current reward is 138.3893272727273.\n",
      "The current reward is 138.53966666666668.\n",
      "The current reward is 138.75882142857142.\n",
      "The current reward is 138.92053982300885.\n",
      "The current reward is 139.15688596491228.\n",
      "The current reward is 139.20962608695652.\n",
      "The current reward is 139.40780172413793.\n",
      "The current reward is 139.59874358974358.\n",
      "The current reward is 139.7482881355932.\n",
      "The current reward is 139.91242016806723.\n",
      "The current reward is 139.96408333333332.\n",
      "The current reward is 139.897826446281.\n",
      "The current reward is 140.0090655737705.\n",
      "The current reward is 140.15740650406505.\n",
      "The current reward is 140.15518548387098.\n",
      "The current reward is 140.226872.\n",
      "The current reward is 140.3202142857143.\n",
      "The current reward is 140.46097637795276.\n",
      "The current reward is 140.4872734375.\n",
      "The current reward is 140.607992248062.\n",
      "The current reward is 140.71768461538463.\n",
      "The current reward is 140.97287022900764.\n",
      "The current reward is 141.22201515151517.\n",
      "The current reward is 141.44324060150376.\n",
      "The current reward is 141.3544776119403.\n",
      "The current reward is 141.50872592592592.\n",
      "The current reward is 141.58313970588236.\n",
      "The current reward is 141.65452554744525.\n",
      "The current reward is 141.69183333333334.\n",
      "The current reward is 141.77522302158275.\n",
      "The current reward is 141.91341428571428.\n",
      "The current reward is 141.9794609929078.\n",
      "The current reward is 142.05702816901407.\n",
      "The current reward is 142.13981818181819.\n",
      "The current reward is 142.11525694444444.\n",
      "The current reward is 142.2185724137931.\n",
      "The current reward is 142.28756164383563.\n",
      "The current reward is 142.39170748299318.\n",
      "The current reward is 142.5243918918919.\n",
      "The current reward is 142.59688590604026.\n",
      "The current reward is 142.63613333333333.\n",
      "The current reward is 142.66387417218544.\n",
      "The current reward is 142.7727894736842.\n",
      "The current reward is 142.85901307189542.\n",
      "The current reward is 142.89824025974025.\n",
      "The current reward is 142.85670322580646.\n",
      "The current reward is 143.02666025641025.\n",
      "The current reward is 143.06364968152866.\n",
      "The current reward is 143.05984810126583.\n",
      "The current reward is 143.0861320754717.\n",
      "The current reward is 143.22300625.\n",
      "The current reward is 143.4033540372671.\n",
      "The current reward is 143.3975061728395.\n",
      "The current reward is 143.51347239263805.\n",
      "The current reward is 143.60526219512195.\n",
      "The current reward is 143.6721393939394.\n",
      "The current reward is 143.77417469879518.\n",
      "The current reward is 143.8832874251497.\n",
      "The current reward is 143.98220833333335.\n",
      "The current reward is 144.0172544378698.\n",
      "The current reward is 144.0571117647059.\n",
      "The current reward is 144.132783625731.\n",
      "The current reward is 144.1816046511628.\n",
      "The current reward is 144.2790809248555.\n",
      "The current reward is 144.33194252873562.\n",
      "The current reward is 144.39703428571428.\n",
      "The current reward is 144.50120454545456.\n",
      "The current reward is 144.59240677966102.\n",
      "The current reward is 144.58550561797753.\n",
      "The current reward is 144.66477094972066.\n",
      "The current reward is 144.80120555555555.\n",
      "The current reward is 144.92167403314917.\n",
      "The current reward is 145.07454395604395.\n",
      "The current reward is 145.15561202185793.\n",
      "The current reward is 145.23833695652175.\n",
      "The current reward is 145.2782216216216.\n",
      "The current reward is 145.3272741935484.\n",
      "The current reward is 145.35912834224598.\n",
      "The current reward is 145.3367925531915.\n",
      "The current reward is 145.39497354497354.\n",
      "The current reward is 145.44091052631578.\n",
      "The current reward is 145.49584816753926.\n",
      "The current reward is 145.46797395833335.\n",
      "The current reward is 145.52684974093265.\n",
      "The current reward is 145.57357216494844.\n",
      "The current reward is 145.5696358974359.\n",
      "The current reward is 145.6968724489796.\n",
      "The current reward is 145.8266040609137.\n",
      "The current reward is 145.91113636363636.\n",
      "The current reward is 145.97998492462312.\n",
      "The current reward is 146.090195.\n",
      "The current reward is 146.16067661691542.\n",
      "The current reward is 146.23815841584158.\n",
      "The current reward is 146.3005763546798.\n",
      "The current reward is 146.37211274509804.\n",
      "The current reward is 146.38456585365853.\n",
      "The current reward is 146.48034466019416.\n",
      "The current reward is 146.51769565217393.\n",
      "The current reward is 146.50551442307693.\n",
      "The current reward is 146.554.\n",
      "The current reward is 146.62399047619047.\n",
      "The current reward is 146.6745308056872.\n",
      "The current reward is 146.74347641509434.\n",
      "The current reward is 146.8342441314554.\n",
      "The current reward is 146.87220560747664.\n",
      "The current reward is 146.97003720930232.\n",
      "The current reward is 147.09201388888889.\n",
      "The current reward is 147.141267281106.\n",
      "The current reward is 147.1311743119266.\n",
      "The current reward is 147.1822283105023.\n",
      "The current reward is 147.2632818181818.\n",
      "The current reward is 147.27022171945703.\n",
      "The current reward is 147.26905855855856.\n",
      "The current reward is 147.3363856502242.\n",
      "The current reward is 147.34208035714286.\n",
      "The current reward is 147.41953777777778.\n",
      "The current reward is 147.5060353982301.\n",
      "The current reward is 147.59448017621145.\n",
      "The current reward is 147.6228596491228.\n",
      "The current reward is 147.6715152838428.\n",
      "The current reward is 147.76983478260868.\n",
      "The current reward is 147.79177489177488.\n",
      "The current reward is 147.83223706896553.\n",
      "The current reward is 147.89088412017168.\n",
      "The current reward is 147.96767948717948.\n",
      "The current reward is 148.0622085106383.\n",
      "The current reward is 148.05582627118645.\n",
      "The current reward is 148.07873417721518.\n",
      "The current reward is 148.15.\n",
      "The current reward is 148.19995815899583.\n",
      "The current reward is 148.29015416666667.\n",
      "The current reward is 148.40691701244813.\n",
      "The current reward is 148.4524090909091.\n",
      "The current reward is 148.53964197530865.\n",
      "The current reward is 148.63193442622952.\n",
      "The current reward is 148.65053469387755.\n",
      "The current reward is 148.73769918699188.\n",
      "The current reward is 148.78328340080972.\n",
      "The current reward is 148.74644758064517.\n",
      "The current reward is 148.83575903614457.\n",
      "The current reward is 148.878952.\n",
      "The current reward is 148.91266135458167.\n",
      "The current reward is 148.9579603174603.\n",
      "The current reward is 149.03801581027668.\n",
      "The current reward is 149.0611968503937.\n",
      "The current reward is 149.11061176470588.\n",
      "The current reward is 149.11803125.\n",
      "The current reward is 149.21853307392996.\n",
      "The current reward is 149.22156976744185.\n",
      "The current reward is 149.26249420849422.\n",
      "The current reward is 149.2905846153846.\n",
      "The current reward is 149.36545210727968.\n",
      "The current reward is 149.43429389312976.\n",
      "The current reward is 149.49526615969583.\n",
      "The current reward is 149.5049356060606.\n",
      "The current reward is 149.5626679245283.\n",
      "The current reward is 149.56427067669173.\n",
      "The current reward is 149.5656329588015.\n",
      "The current reward is 149.58293656716418.\n",
      "The current reward is 149.59463568773234.\n",
      "The current reward is 149.62672592592594.\n",
      "The current reward is 149.69885977859778.\n",
      "The current reward is 149.77354779411766.\n",
      "The current reward is 149.8182271062271.\n",
      "The current reward is 149.8426423357664.\n",
      "The current reward is 149.91626909090908.\n",
      "The current reward is 149.9429927536232.\n",
      "The current reward is 149.9829855595668.\n",
      "The current reward is 150.00385971223022.\n",
      "The current reward is 150.0686164874552.\n",
      "The current reward is 150.14968214285713.\n",
      "The current reward is 150.11309608540924.\n",
      "The current reward is 150.10906382978723.\n",
      "The current reward is 150.18756183745583.\n",
      "The current reward is 150.24079929577465.\n",
      "The current reward is 150.25450526315788.\n",
      "The current reward is 150.19420979020978.\n",
      "The current reward is 150.25218118466898.\n",
      "The current reward is 150.3043298611111.\n",
      "The current reward is 150.3293114186851.\n",
      "The current reward is 150.37409310344827.\n",
      "The current reward is 150.37949484536082.\n",
      "The current reward is 150.43110958904109.\n",
      "The current reward is 150.47715699658704.\n"
     ]
    }
   ],
   "source": [
    "# 3.1 [20 pts]\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)\n",
    "\n",
    "def get_action(state: tuple[int, int, int, int]):\n",
    "    \"\"\"Get action based on epsilon greedy\"\"\"\n",
    "    if np.random.random() < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return int(np.argmax(Q[state]))\n",
    "    \n",
    "def update(\n",
    "        state: tuple[int, int, int, int],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_state: tuple[int, int, int, int],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(Q[next_state])\n",
    "        temporal_difference = (\n",
    "            reward + gamma * future_q_value - Q[state][action]\n",
    "        )\n",
    "\n",
    "        Q[state][action] = (\n",
    "            Q[state][action] + alpha * temporal_difference\n",
    "        )\n",
    "\n",
    "\n",
    "rewards = 0\n",
    "num_episodes = 0\n",
    "while True:\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    num_episodes += 1\n",
    "\n",
    "    while not done:\n",
    "        s = obs2bin(obs)\n",
    "        action = get_action(s)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_s = obs2bin(next_obs)\n",
    "\n",
    "        update(s, action, reward, terminated, next_s)\n",
    "        rewards += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    if num_episodes % log_n == 0:\n",
    "        print(\"The current reward is {}.\".format(rewards/num_episodes))\n",
    "        # env.render()\n",
    "\n",
    "    if rewards / num_episodes > 150.5:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 pts] Experiments\n",
    "\n",
    "Given a working algorithm, you will run a few experiments.  Either make a copy of your code above to modify, or make the modifications in a way that they can be commented out or switched between (with boolean flag if statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. [5 pts] $\\epsilon$-greedy.**  How sensitive are the results to the value of $\\epsilon$?   First, write down your prediction of what would happen if $\\epsilon$ is set to various values, including for example [0, 0.05, 0.25, 0.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\epsilon=0$, there will be no increasing of reward as there is no exploration and Q is set to all 0s. The policy will choose a fixed action every time, which is suboptimal.\n",
    "\n",
    "For $\\epsilon=0.05$, there will be some exploration. The training will proceed. Reward will increase.\n",
    "\n",
    "As $\\epsilon$ increases, the exploration increases during training. The training speed at first might be slower because of the exploration, but later when it is closer to converge, the policy will provide more accurate actions. However, too high of $\\epsilon$ can result in instability of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the experiment and observe the impact on the algorithm.  Report the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, no exploration will result in a suboptimal policy. The average reward will stuck at 9.4.\n",
    "\n",
    "For $\\epsilon=0.05$ the rewards accumulate a lot faster at the beginning compared to higher $\\epsilon$. This is expected as well since higher $\\epsilon$ is doing more exploration than exploitation.\n",
    "\n",
    "On a 30mins training, 0.05 session reaches avg reward of 150. 0.25 session only reaches 102.\n",
    "\n",
    "When $\\epsilon$ is too high, i.e. 0.25 or 0.5, it's really hard for the policy to converge.\n",
    "\n",
    "0.01 - 0.1 should be a reasonable range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. [5 pts] Design your own experiment.** Design a modification that you think would either increase or reduce performance.  A simple example (which you can use) is initializing the Q-table differently, and thinking about how this might alter performance. Write down your idea, what you think might happen, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My modification is to increase the number of bins. The discretization could impact the result quite a lot. If the number of bin is too low, there might not be enough discretized states to represent the continuous states, i.e. the accuracy of the discretization is low. If the number of bins are too high, the training cost (memory and speed) will increase tremendously due to the exponential increasing of the Q table size. This will also harm since many of the states will result in similar actions so too many states will not increase the performance.\n",
    "\n",
    "Personally, I think 10 bins didn't perform well on my machine so I decided to try out 20, 30, 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out #bins = 30 gives the best performance (speed and avg total reward).\n",
    "\n",
    "10 somehow stuck at around 135 and increases very slowly after that.\n",
    "\n",
    "40 trains too slow for the entire time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A. Extensions (fully optional, will not be graded, if you have time after Part 2)\n",
    "\n",
    "- plots your learning curve, using e.g., matplotlib \n",
    "- visualize the Q-table to see which values are being updated and not\n",
    "- design a better binning strategy that uses fewer bins for a better-performing policy\n",
    "- extend this approach to work on different environments (e.g., LunarLander-v2)\n",
    "- extend this approach to work on environments with continuous actions, by using a fixed set of discrete samples of the action space.  e.g., for Pendulum-v0\n",
    "- implement a simple deep learning version of this.  we will see next part that DQN uses some tricks to make the neural network training more stable.  Experiment directly with simply replacing the Q-table with a Q-Network and train the Q-Network using gradient descent with `loss = (targets - Q(s,a))**2`, where `targets = stop_grad(R + gamma * maxa(Q(s,a))`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 [60 pts] Behavioral Cloning and Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The second part of assignment will help you transition from tabular approaches to deep neural network approaches. You will implement the [Atari DQN / Deep Q-Learning](https://arxiv.org/abs/1312.5602) algorithm, which arguably kicked off the modern Deep Reinforcement Learning craze.\n",
    "\n",
    "In this part we will use PyTorch as our deep learning framework.  To familiarize yourself with PyTorch, your first task is to use a behavior cloning (BC) approach to learn a policy.  Behavior cloning is a supervised learning method in which there exists a dataset of expert demonstrations (state-action pairs) and the goal is to learn a policy $\\pi$ that mimics this expert.  At any given state, your policy should choose the same action the export would.\n",
    "\n",
    "Since BC avoids the need to collect data from the policy you are trying to learn, it is relatively simple. \n",
    "This makes it a nice stepping stone for implementing DQN. Furthermore, BC is relevant to modern approaches---for example its use as an initialization for systems like [AlphaGo][go] and [AlphaStar][star], which then use RL to further adapte the BC result.  \n",
    "\n",
    "<!--\n",
    "\n",
    "I feel like this might be better suited to going lower in the document:\n",
    "\n",
    "Unfortunately, in many tasks it is impossible to collect good expert demonstrations, making\n",
    "\n",
    "it's not always possible to have good expert demonstrations for a task in an environemnt and this is where reinforcement learning comes handy. Through the reward signal retrieved by interacting with the environment, the agent learns by itself what is a good policy and can learn to outperform the experts.\n",
    "\n",
    "-->\n",
    "\n",
    "Goals:\n",
    "- Famliarize yourself with PyTorch and its API including models, datasets, dataloaders\n",
    "- Implement a supervised learning approach (behavioral cloning) to learn a policy.\n",
    "- Implement the DQN objective and learn a policy through environment interaction.\n",
    "\n",
    "[go]:  https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[star]: https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii\n",
    "\n",
    "## Submission information\n",
    "\n",
    "- Complete by editing and executing the associated Python files.\n",
    "- Copy and paste the code and the terminal output requested in the predefined cells on this Jupyter notebook.\n",
    "- When done, upload the completed Jupyter notebook (ipynb file) on canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "If you have never used PyTorch before, we recommend you follow this [60 Minutes Blitz][blitz] tutorial from the official website. It should give you enough context to be able to complete the assignment.\n",
    "\n",
    "\n",
    "**If you have issues, post questions to Piazza**\n",
    "\n",
    "### Installation\n",
    "\n",
    "To install all required python packages:\n",
    "\n",
    "```\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Debugging\n",
    "\n",
    "\n",
    "You can include:  `import ipdb; ipdb.set_trace()` in your code and it will drop you to that point in the code, where you can interact with variables and test out expressions.  We recommend this as an effective method to debug the algorithms.\n",
    "\n",
    "\n",
    "[blitz]: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [36 pts] Behavioral Cloning\n",
    "\n",
    "Behavioral Cloning is a type of supervised learning in which you are given a dataset of expert demonstrations tuple $(s, a)$ and the goal is to learn a policy function $\\hat a = \\pi(s)$, such that $\\hat a = a$.\n",
    "\n",
    "The optimization objective is $\\min_\\theta D(\\pi(s), a)$ where $\\theta$ are the parameters the policy $\\pi$, in our case the weights of a neural network, and where $D$ represents some difference between the actions.\n",
    "\n",
    "---\n",
    "\n",
    "Before starting, we suggest reading through the provided files.\n",
    "\n",
    "For Behavioral Cloning, the important files to understand are: `model.py`, `dataset.py` and `bc.py`.\n",
    "\n",
    "- The file `model.py` has the skeleton for the model (which you will have to complete in the following questions),\n",
    "\n",
    "- The file `dataset.py` has the skeleton for the dataset the model is being trained with,\n",
    "\n",
    "- and, `bc.py` will have all the structure for training the model with the dataset.\n",
    "\n",
    "\n",
    "### [10 pts] 1.1 Dataset\n",
    "\n",
    "We provide a pickle file with pre-collected expert demonstrations on CartPole from which to learn the policy $\\pi$. The data has been collected from an expert policy on the environment, with the addition of a small amount of gaussian noise to the actions.\n",
    "\n",
    "The pickle file contains a list of tuples of states and actions in `numpy` in the following way:\n",
    "\n",
    "```\n",
    "[(state s, action a), (state s, action a), (state s, action a), ...]\n",
    "```\n",
    "\n",
    "In the `dataset.py` file, we provide skeleton code for creating a custom dataset. The provided code shows how to load the file.\n",
    "\n",
    "Your goal is to overwrite the `__getitem__` function in order to return a dictionary of tensors of the correct type.\n",
    "\n",
    "Hint: Look in the `bc.py` file to understand how the dataset is used.\n",
    "\n",
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 pts]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR __getitem__ method here\n",
    "def __getitem__(self, index):\n",
    "    item = self.data[index]\n",
    "    return {\"state\": item[0], \"action\": item[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99660, (99660, 4), (99660,), array([[2.39948596, 1.84697975, 0.14641718, 0.47143314]]), array([[-0.72267057, -0.43303689, -0.05007198, -0.38122098]]), array([1], dtype=int64), array([0], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "from dataset import Dataset\n",
    "\n",
    "myDataset = Dataset(\"./CartPole-v1_dataset.pkl\")\n",
    "\n",
    "\n",
    "# def getDimensions(self):\n",
    "#         states = []\n",
    "#         actions = []\n",
    "#         for d in self.data:\n",
    "#             states.append(d[0])\n",
    "#             actions.append(d[1])\n",
    "        \n",
    "#         states = np.array(states)\n",
    "#         actions = np.array(actions)\n",
    "#         return len(states), states.shape, actions.shape, np.max(states, axis=0, keepdims=True), np.min(states, axis=0, keepdims=True), np.max(actions,axis=0,keepdims=True), np.min(actions,axis=0,keepdims=True)\n",
    "\n",
    "\n",
    "print(myDataset.getDimensions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 pt]** How big is the dataset provided?\n",
    "\n",
    "99660\n",
    "\n",
    "**[2 pts]** What is the dimensionality of $s$ and what range does each dimension of $s$ span?  I.e., how much of the state space does the expert data cover? What are the dimensionalities and ranges of the action $a$ in the dataset (how much of the action space does the expert data cover)?\n",
    "\n",
    "From the previous cell, we can see for the states\n",
    "\n",
    "- 4 dimension\n",
    "- each value in the state spans [-0.72267057, -0.43303689, -0.05007198, -0.38122098] to [2.39948596, 1.84697975, 0.14641718, 0.47143314]\n",
    "\n",
    "for the actions\n",
    "\n",
    "- 1 dimension\n",
    "- spans 0 to 1 (both actions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] 1.2 Environment\n",
    "\n",
    "Recall the state and action space of CartPole, from the previous assignment.\n",
    "\n",
    "Considering the full state and action spaces, do you think the provided expert dataset has good coverage?  Why or why not? How might this impact the performance of our cloned policy?\n",
    "\n",
    "The full state space from the definition is -4.8 to 4.9 for positions, -inf to inf for velocities and -0.418 to 0.418 for angles. However, the termination condition is for those are different. For positions, they are (-2.4, 2.4). For angles, they are (-0.2095,0.2095). In the expert data, the positions are from -0.7 to 2.4, which spans most of the state space (before termination) but not very much the left side to the limit. Similar for the angles. Both actions are coverd. I think the expert policy can handle the situations when the pole are leaning right well but not leaning left. If the expert data lack data for part of the state space, the cloned policy will do even worse because the error accumulates during time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [14 pts] 1.3 Model\n",
    "\n",
    "The file `model.py` provides skeleton code for the model. Your goal is to create the architecture of the network by adding layers that map the input to output.\n",
    "\n",
    "You will need to update the `__init__` method and the `forward` method.\n",
    "\n",
    "The `select_action` method has already been written for you.  This should be used when running the policy in the environment, while the `forward` function should be used at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[10 pts]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR MyModel class here\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        # TODO YOUR CODE HERE FOR INITIALIZING THE MODEL\n",
    "        # Guidelines for network size: start with 2 hidden layers and maximum 32 neurons per layer\n",
    "        # feel free to explore different sizes\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_size, 256, True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256, True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_size, True),\n",
    "            )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(torch.flatten(x, 1))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.eval()\n",
    "        x = self.forward(state)\n",
    "        self.train()\n",
    "        return x.max(1)[1].view(1, 1).to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "- **[2 pts]** What is the dimension and meaning of the input of the network\n",
    "\n",
    "Input size is 4, this is the state size.\n",
    "\n",
    "- **[2 pts]** Similarly, describe the output.\n",
    "\n",
    "There are 2 hidden layers with 256 neurons each and the final layer just maps the output to 2 actions with a log likelihood softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [7 pts] 1.4 Training\n",
    "\n",
    "The file `bc.py` is the entry point for training your behavioral cloning model. The skeleton and the main components are already there.\n",
    "\n",
    "The missing parts for you to do are:\n",
    "\n",
    "- Initializing the model\n",
    "- Choosing a loss function\n",
    "- Choosing an optimizer\n",
    "- Playing with hyperparameters to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[5 pts]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER FOR YOUR CODE HER\n",
    "# HOW DID YOU INITIALIZE YOUR MODEL, OPTIMIZER AND LOSS FUNCTIONS? PASTE HERE YOUR FINAL CODE\n",
    "# NOTE: YOU CAN KEEP THE FOLLOWING LINES COMMENTED OUT, AS RUNNING THIS CELL WILL PROBABLY RESULT IN ERRORS\n",
    "\n",
    "model = MyModel(state_size=4, action_size=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 bc.py\n",
    "```\n",
    "\n",
    "**During all of this assignment, the code in `eval_policy.py` will be your best friend.** At any time, you can test your model by giving as argument the path to the model weights and the environment name using the following command:\n",
    "\n",
    "```\n",
    "python3 eval_policy.py --model-path /path/to/model/weights --env ENV_NAME\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10\n",
    "[epoch    1/100] [iter       0] [loss 0.69814]\n",
    "[epoch    1/100] [iter     500] [loss 0.04178]\n",
    "[epoch    1/100] [iter    1000] [loss 0.02584]\n",
    "[epoch    1/100] [iter    1500] [loss 0.02598]\n",
    "[epoch    2/100] [iter    2000] [loss 0.01365]\n",
    "[epoch    2/100] [iter    2500] [loss 0.00593]\n",
    "[epoch    2/100] [iter    3000] [loss 0.00375]\n",
    "[Test on environment] [epoch 2/100] [score 244.20]\n",
    "[epoch    3/100] [iter    3500] [loss 0.00397]\n",
    "[epoch    3/100] [iter    4000] [loss 0.00393]\n",
    "[epoch    3/100] [iter    4500] [loss 0.01878]\n",
    "[epoch    4/100] [iter    5000] [loss 0.00223]\n",
    "[epoch    4/100] [iter    5500] [loss 0.01809]\n",
    "[epoch    4/100] [iter    6000] [loss 0.04941]\n",
    "[Test on environment] [epoch 4/100] [score 232.30]\n",
    "[epoch    5/100] [iter    6500] [loss 0.00287]\n",
    "[epoch    5/100] [iter    7000] [loss 0.00088]\n",
    "[epoch    5/100] [iter    7500] [loss 0.02178]\n",
    "[epoch    6/100] [iter    8000] [loss 0.00364]\n",
    "[epoch    6/100] [iter    8500] [loss 0.00400]\n",
    "[epoch    6/100] [iter    9000] [loss 0.04437]\n",
    "[Test on environment] [epoch 6/100] [score 217.50]\n",
    "[epoch    7/100] [iter    9500] [loss 0.01728]\n",
    "[epoch    7/100] [iter   10000] [loss 0.00480]\n",
    "[epoch    7/100] [iter   10500] [loss 0.03154]\n",
    "[epoch    8/100] [iter   11000] [loss 0.00059]\n",
    "[epoch    8/100] [iter   11500] [loss 0.02310]\n",
    "[epoch    8/100] [iter   12000] [loss 0.00040]\n",
    "[Test on environment] [epoch 8/100] [score 224.00]\n",
    "[epoch    9/100] [iter   12500] [loss 0.02389]\n",
    "[epoch    9/100] [iter   13000] [loss 0.00175]\n",
    "[epoch    9/100] [iter   13500] [loss 0.02723]\n",
    "[epoch    9/100] [iter   14000] [loss 0.00690]\n",
    "[epoch   10/100] [iter   14500] [loss 0.05248]\n",
    "[epoch   10/100] [iter   15000] [loss 0.02820]\n",
    "[epoch   10/100] [iter   15500] [loss 0.02270]\n",
    "[Test on environment] [epoch 10/100] [score 275.30]\n",
    "[epoch   11/100] [iter   16000] [loss 0.01192]\n",
    "[epoch   11/100] [iter   16500] [loss 0.01329]\n",
    "[epoch   11/100] [iter   17000] [loss 0.00225]\n",
    "[epoch   12/100] [iter   17500] [loss 0.00082]\n",
    "[epoch   12/100] [iter   18000] [loss 0.00377]\n",
    "[epoch   12/100] [iter   18500] [loss 0.00528]\n",
    "[Test on environment] [epoch 12/100] [score 263.80]\n",
    "[epoch   13/100] [iter   19000] [loss 0.00684]\n",
    "[epoch   13/100] [iter   19500] [loss 0.01661]\n",
    "[epoch   13/100] [iter   20000] [loss 0.00034]\n",
    "[epoch   14/100] [iter   20500] [loss 0.00059]\n",
    "[epoch   14/100] [iter   21000] [loss 0.00110]\n",
    "[epoch   14/100] [iter   21500] [loss 0.00816]\n",
    "[Test on environment] [epoch 14/100] [score 241.60]\n",
    "[epoch   15/100] [iter   22000] [loss 0.00062]\n",
    "[epoch   15/100] [iter   22500] [loss 0.00228]\n",
    "[epoch   15/100] [iter   23000] [loss 0.00029]\n",
    "[epoch   16/100] [iter   23500] [loss 0.01094]\n",
    "[epoch   16/100] [iter   24000] [loss 0.00026]\n",
    "[epoch   16/100] [iter   24500] [loss 0.05375]\n",
    "[Test on environment] [epoch 16/100] [score 239.40]\n",
    "[epoch   17/100] [iter   25000] [loss 0.00177]\n",
    "[epoch   17/100] [iter   25500] [loss 0.00086]\n",
    "[epoch   17/100] [iter   26000] [loss 0.03156]\n",
    "[epoch   18/100] [iter   26500] [loss 0.01736]\n",
    "[epoch   18/100] [iter   27000] [loss 0.01559]\n",
    "[epoch   18/100] [iter   27500] [loss 0.00058]\n",
    "[epoch   18/100] [iter   28000] [loss 0.00420]\n",
    "[Test on environment] [epoch 18/100] [score 280.70]\n",
    "[epoch   19/100] [iter   28500] [loss 0.00018]\n",
    "[epoch   19/100] [iter   29000] [loss 0.00095]\n",
    "[epoch   19/100] [iter   29500] [loss 0.00021]\n",
    "[epoch   20/100] [iter   30000] [loss 0.01053]\n",
    "[epoch   20/100] [iter   30500] [loss 0.00095]\n",
    "[epoch   20/100] [iter   31000] [loss 0.00593]\n",
    "[Test on environment] [epoch 20/100] [score 284.20]\n",
    "[epoch   21/100] [iter   31500] [loss 0.01220]\n",
    "[epoch   21/100] [iter   32000] [loss 0.01184]\n",
    "[epoch   21/100] [iter   32500] [loss 0.00941]\n",
    "[epoch   22/100] [iter   33000] [loss 0.00003]\n",
    "[epoch   22/100] [iter   33500] [loss 0.00539]\n",
    "[epoch   22/100] [iter   34000] [loss 0.00616]\n",
    "[Test on environment] [epoch 22/100] [score 243.70]\n",
    "[epoch   23/100] [iter   34500] [loss 0.00131]\n",
    "[epoch   23/100] [iter   35000] [loss 0.00922]\n",
    "[epoch   23/100] [iter   35500] [loss 0.00979]\n",
    "[epoch   24/100] [iter   36000] [loss 0.00263]\n",
    "[epoch   24/100] [iter   36500] [loss 0.00003]\n",
    "[epoch   24/100] [iter   37000] [loss 0.00321]\n",
    "[Test on environment] [epoch 24/100] [score 270.40]\n",
    "[epoch   25/100] [iter   37500] [loss 0.02302]\n",
    "[epoch   25/100] [iter   38000] [loss 0.03020]\n",
    "[epoch   25/100] [iter   38500] [loss 0.00001]\n",
    "[epoch   26/100] [iter   39000] [loss 0.00005]\n",
    "[epoch   26/100] [iter   39500] [loss 0.02087]\n",
    "[epoch   26/100] [iter   40000] [loss 0.00167]\n",
    "[epoch   26/100] [iter   40500] [loss 0.01376]\n",
    "[Test on environment] [epoch 26/100] [score 281.00]\n",
    "[epoch   27/100] [iter   41000] [loss 0.01028]\n",
    "[epoch   27/100] [iter   41500] [loss 0.04407]\n",
    "[epoch   27/100] [iter   42000] [loss 0.00744]\n",
    "[epoch   28/100] [iter   42500] [loss 0.00174]\n",
    "[epoch   28/100] [iter   43000] [loss 0.00367]\n",
    "[epoch   28/100] [iter   43500] [loss 0.01550]\n",
    "[Test on environment] [epoch 28/100] [score 271.00]\n",
    "[epoch   29/100] [iter   44000] [loss 0.00194]\n",
    "[epoch   29/100] [iter   44500] [loss 0.00093]\n",
    "[epoch   29/100] [iter   45000] [loss 0.00076]\n",
    "[epoch   30/100] [iter   45500] [loss 0.00044]\n",
    "[epoch   30/100] [iter   46000] [loss 0.00051]\n",
    "[epoch   30/100] [iter   46500] [loss 0.01051]\n",
    "[Test on environment] [epoch 30/100] [score 270.70]\n",
    "[epoch   31/100] [iter   47000] [loss 0.00302]\n",
    "[epoch   31/100] [iter   47500] [loss 0.00044]\n",
    "[epoch   31/100] [iter   48000] [loss 0.00029]\n",
    "[epoch   32/100] [iter   48500] [loss 0.00064]\n",
    "[epoch   32/100] [iter   49000] [loss 0.00812]\n",
    "[epoch   32/100] [iter   49500] [loss 0.00023]\n",
    "[Test on environment] [epoch 32/100] [score 278.10]\n",
    "[epoch   33/100] [iter   50000] [loss 0.02734]\n",
    "[epoch   33/100] [iter   50500] [loss 0.00443]\n",
    "[epoch   33/100] [iter   51000] [loss 0.02590]\n",
    "[epoch   34/100] [iter   51500] [loss 0.01302]\n",
    "[epoch   34/100] [iter   52000] [loss 0.00167]\n",
    "[epoch   34/100] [iter   52500] [loss 0.00004]\n",
    "[Test on environment] [epoch 34/100] [score 237.90]\n",
    "[epoch   35/100] [iter   53000] [loss 0.00012]\n",
    "[epoch   35/100] [iter   53500] [loss 0.00377]\n",
    "[epoch   35/100] [iter   54000] [loss 0.00012]\n",
    "[epoch   35/100] [iter   54500] [loss 0.02393]\n",
    "[epoch   36/100] [iter   55000] [loss 0.00411]\n",
    "[epoch   36/100] [iter   55500] [loss 0.00299]\n",
    "[epoch   36/100] [iter   56000] [loss 0.00000]\n",
    "[Test on environment] [epoch 36/100] [score 233.30]\n",
    "[epoch   37/100] [iter   56500] [loss 0.00110]\n",
    "[epoch   37/100] [iter   57000] [loss 0.00020]\n",
    "[epoch   37/100] [iter   57500] [loss 0.00058]\n",
    "[epoch   38/100] [iter   58000] [loss 0.00001]\n",
    "[epoch   38/100] [iter   58500] [loss 0.04285]\n",
    "[epoch   38/100] [iter   59000] [loss 0.00010]\n",
    "[Test on environment] [epoch 38/100] [score 247.60]\n",
    "[epoch   39/100] [iter   59500] [loss 0.00057]\n",
    "[epoch   39/100] [iter   60000] [loss 0.00195]\n",
    "[epoch   39/100] [iter   60500] [loss 0.01053]\n",
    "[epoch   40/100] [iter   61000] [loss 0.00049]\n",
    "[epoch   40/100] [iter   61500] [loss 0.00113]\n",
    "[epoch   40/100] [iter   62000] [loss 0.02074]\n",
    "[Test on environment] [epoch 40/100] [score 233.90]\n",
    "[epoch   41/100] [iter   62500] [loss 0.04126]\n",
    "[epoch   41/100] [iter   63000] [loss 0.00806]\n",
    "[epoch   41/100] [iter   63500] [loss 0.00000]\n",
    "[epoch   42/100] [iter   64000] [loss 0.00072]\n",
    "[epoch   42/100] [iter   64500] [loss 0.00162]\n",
    "[epoch   42/100] [iter   65000] [loss 0.00012]\n",
    "[Test on environment] [epoch 42/100] [score 264.40]\n",
    "[epoch   43/100] [iter   65500] [loss 0.00008]\n",
    "[epoch   43/100] [iter   66000] [loss 0.02011]\n",
    "[epoch   43/100] [iter   66500] [loss 0.00007]\n",
    "[epoch   44/100] [iter   67000] [loss 0.00026]\n",
    "[epoch   44/100] [iter   67500] [loss 0.02528]\n",
    "[epoch   44/100] [iter   68000] [loss 0.00000]\n",
    "[epoch   44/100] [iter   68500] [loss 0.00021]\n",
    "[Test on environment] [epoch 44/100] [score 250.90]\n",
    "[epoch   45/100] [iter   69000] [loss 0.00012]\n",
    "[epoch   45/100] [iter   69500] [loss 0.00069]\n",
    "[epoch   45/100] [iter   70000] [loss 0.00773]\n",
    "[epoch   46/100] [iter   70500] [loss 0.00007]\n",
    "[epoch   46/100] [iter   71000] [loss 0.00142]\n",
    "[epoch   46/100] [iter   71500] [loss 0.00000]\n",
    "[Test on environment] [epoch 46/100] [score 244.60]\n",
    "[epoch   47/100] [iter   72000] [loss 0.00170]\n",
    "[epoch   47/100] [iter   72500] [loss 0.00913]\n",
    "[epoch   47/100] [iter   73000] [loss 0.00017]\n",
    "[epoch   48/100] [iter   73500] [loss 0.00771]\n",
    "[epoch   48/100] [iter   74000] [loss 0.01189]\n",
    "[epoch   48/100] [iter   74500] [loss 0.01658]\n",
    "[Test on environment] [epoch 48/100] [score 241.60]\n",
    "[epoch   49/100] [iter   75000] [loss 0.00004]\n",
    "[epoch   49/100] [iter   75500] [loss 0.00094]\n",
    "[epoch   49/100] [iter   76000] [loss 0.00926]\n",
    "[epoch   50/100] [iter   76500] [loss 0.00091]\n",
    "[epoch   50/100] [iter   77000] [loss 0.04065]\n",
    "[epoch   50/100] [iter   77500] [loss 0.03460]\n",
    "[Test on environment] [epoch 50/100] [score 273.60]\n",
    "[epoch   51/100] [iter   78000] [loss 0.00004]\n",
    "[epoch   51/100] [iter   78500] [loss 0.00000]\n",
    "[epoch   51/100] [iter   79000] [loss 0.03253]\n",
    "[epoch   52/100] [iter   79500] [loss 0.00001]\n",
    "[epoch   52/100] [iter   80000] [loss 0.00000]\n",
    "[epoch   52/100] [iter   80500] [loss 0.00002]\n",
    "[epoch   52/100] [iter   81000] [loss 0.00001]\n",
    "[Test on environment] [epoch 52/100] [score 254.50]\n",
    "[epoch   53/100] [iter   81500] [loss 0.00001]\n",
    "[epoch   53/100] [iter   82000] [loss 0.00126]\n",
    "[epoch   53/100] [iter   82500] [loss 0.00116]\n",
    "[epoch   54/100] [iter   83000] [loss 0.00572]\n",
    "[epoch   54/100] [iter   83500] [loss 0.00008]\n",
    "[epoch   54/100] [iter   84000] [loss 0.00428]\n",
    "[Test on environment] [epoch 54/100] [score 272.20]\n",
    "[epoch   55/100] [iter   84500] [loss 0.00014]\n",
    "[epoch   55/100] [iter   85000] [loss 0.00001]\n",
    "[epoch   55/100] [iter   85500] [loss 0.00062]\n",
    "[epoch   56/100] [iter   86000] [loss 0.00920]\n",
    "[epoch   56/100] [iter   86500] [loss 0.00044]\n",
    "[epoch   56/100] [iter   87000] [loss 0.00239]\n",
    "[Test on environment] [epoch 56/100] [score 264.50]\n",
    "[epoch   57/100] [iter   87500] [loss 0.00141]\n",
    "[epoch   57/100] [iter   88000] [loss 0.00007]\n",
    "[epoch   57/100] [iter   88500] [loss 0.00124]\n",
    "[epoch   58/100] [iter   89000] [loss 0.02416]\n",
    "[epoch   58/100] [iter   89500] [loss 0.00025]\n",
    "[epoch   58/100] [iter   90000] [loss 0.00050]\n",
    "[Test on environment] [epoch 58/100] [score 285.60]\n",
    "[epoch   59/100] [iter   90500] [loss 0.02778]\n",
    "[epoch   59/100] [iter   91000] [loss 0.00128]\n",
    "[epoch   59/100] [iter   91500] [loss 0.00183]\n",
    "[epoch   60/100] [iter   92000] [loss 0.00000]\n",
    "[epoch   60/100] [iter   92500] [loss 0.00092]\n",
    "[epoch   60/100] [iter   93000] [loss 0.00003]\n",
    "[Test on environment] [epoch 60/100] [score 252.60]\n",
    "[epoch   61/100] [iter   93500] [loss 0.00004]\n",
    "[epoch   61/100] [iter   94000] [loss 0.01092]\n",
    "[epoch   61/100] [iter   94500] [loss 0.00002]\n",
    "[epoch   61/100] [iter   95000] [loss 0.00007]\n",
    "[epoch   62/100] [iter   95500] [loss 0.00001]\n",
    "[epoch   62/100] [iter   96000] [loss 0.00918]\n",
    "[epoch   62/100] [iter   96500] [loss 0.00093]\n",
    "[Test on environment] [epoch 62/100] [score 278.40]\n",
    "[epoch   63/100] [iter   97000] [loss 0.00643]\n",
    "[epoch   63/100] [iter   97500] [loss 0.00033]\n",
    "[epoch   63/100] [iter   98000] [loss 0.00000]\n",
    "[epoch   64/100] [iter   98500] [loss 0.00000]\n",
    "[epoch   64/100] [iter   99000] [loss 0.00172]\n",
    "[epoch   64/100] [iter   99500] [loss 0.00308]\n",
    "[Test on environment] [epoch 64/100] [score 253.80]\n",
    "[epoch   65/100] [iter  100000] [loss 0.00003]\n",
    "[epoch   65/100] [iter  100500] [loss 0.01033]\n",
    "[epoch   65/100] [iter  101000] [loss 0.00693]\n",
    "[epoch   66/100] [iter  101500] [loss 0.00310]\n",
    "[epoch   66/100] [iter  102000] [loss 0.02156]\n",
    "[epoch   66/100] [iter  102500] [loss 0.00001]\n",
    "[Test on environment] [epoch 66/100] [score 240.30]\n",
    "[epoch   67/100] [iter  103000] [loss 0.00134]\n",
    "[epoch   67/100] [iter  103500] [loss 0.00178]\n",
    "[epoch   67/100] [iter  104000] [loss 0.00802]\n",
    "[epoch   68/100] [iter  104500] [loss 0.00000]\n",
    "[epoch   68/100] [iter  105000] [loss 0.00715]\n",
    "[epoch   68/100] [iter  105500] [loss 0.04453]\n",
    "[Test on environment] [epoch 68/100] [score 299.80]\n",
    "[epoch   69/100] [iter  106000] [loss 0.00032]\n",
    "[epoch   69/100] [iter  106500] [loss 0.00006]\n",
    "[epoch   69/100] [iter  107000] [loss 0.00172]\n",
    "[epoch   69/100] [iter  107500] [loss 0.00094]\n",
    "[epoch   70/100] [iter  108000] [loss 0.05337]\n",
    "[epoch   70/100] [iter  108500] [loss 0.00070]\n",
    "[epoch   70/100] [iter  109000] [loss 0.00924]\n",
    "[Test on environment] [epoch 70/100] [score 224.40]\n",
    "[epoch   71/100] [iter  109500] [loss 0.02305]\n",
    "[epoch   71/100] [iter  110000] [loss 0.00047]\n",
    "[epoch   71/100] [iter  110500] [loss 0.03233]\n",
    "[epoch   72/100] [iter  111000] [loss 0.00001]\n",
    "[epoch   72/100] [iter  111500] [loss 0.00098]\n",
    "[epoch   72/100] [iter  112000] [loss 0.00687]\n",
    "[Test on environment] [epoch 72/100] [score 240.20]\n",
    "[epoch   73/100] [iter  112500] [loss 0.00428]\n",
    "[epoch   73/100] [iter  113000] [loss 0.01054]\n",
    "[epoch   73/100] [iter  113500] [loss 0.04568]\n",
    "[epoch   74/100] [iter  114000] [loss 0.00000]\n",
    "[epoch   74/100] [iter  114500] [loss 0.00031]\n",
    "[epoch   74/100] [iter  115000] [loss 0.03074]\n",
    "[Test on environment] [epoch 74/100] [score 248.60]\n",
    "[epoch   75/100] [iter  115500] [loss 0.00001]\n",
    "[epoch   75/100] [iter  116000] [loss 0.02712]\n",
    "[epoch   75/100] [iter  116500] [loss 0.00002]\n",
    "[epoch   76/100] [iter  117000] [loss 0.00000]\n",
    "[epoch   76/100] [iter  117500] [loss 0.03004]\n",
    "[epoch   76/100] [iter  118000] [loss 0.00002]\n",
    "[Test on environment] [epoch 76/100] [score 257.10]\n",
    "[epoch   77/100] [iter  118500] [loss 0.00019]\n",
    "[epoch   77/100] [iter  119000] [loss 0.01276]\n",
    "[epoch   77/100] [iter  119500] [loss 0.00005]\n",
    "[epoch   78/100] [iter  120000] [loss 0.00001]\n",
    "[epoch   78/100] [iter  120500] [loss 0.00758]\n",
    "[epoch   78/100] [iter  121000] [loss 0.00019]\n",
    "[epoch   78/100] [iter  121500] [loss 0.00233]\n",
    "[Test on environment] [epoch 78/100] [score 293.80]\n",
    "[epoch   79/100] [iter  122000] [loss 0.02229]\n",
    "[epoch   79/100] [iter  122500] [loss 0.00755]\n",
    "[epoch   79/100] [iter  123000] [loss 0.00759]\n",
    "[epoch   80/100] [iter  123500] [loss 0.00371]\n",
    "[epoch   80/100] [iter  124000] [loss 0.00000]\n",
    "[epoch   80/100] [iter  124500] [loss 0.00020]\n",
    "[Test on environment] [epoch 80/100] [score 261.50]\n",
    "[epoch   81/100] [iter  125000] [loss 0.00083]\n",
    "[epoch   81/100] [iter  125500] [loss 0.00000]\n",
    "[epoch   81/100] [iter  126000] [loss 0.00660]\n",
    "[epoch   82/100] [iter  126500] [loss 0.00004]\n",
    "[epoch   82/100] [iter  127000] [loss 0.01092]\n",
    "[epoch   82/100] [iter  127500] [loss 0.00170]\n",
    "[Test on environment] [epoch 82/100] [score 237.30]\n",
    "[epoch   83/100] [iter  128000] [loss 0.00002]\n",
    "[epoch   83/100] [iter  128500] [loss 0.00003]\n",
    "[epoch   83/100] [iter  129000] [loss 0.00002]\n",
    "[epoch   84/100] [iter  129500] [loss 0.00689]\n",
    "[epoch   84/100] [iter  130000] [loss 0.00906]\n",
    "[epoch   84/100] [iter  130500] [loss 0.00001]\n",
    "[Test on environment] [epoch 84/100] [score 261.70]\n",
    "[epoch   85/100] [iter  131000] [loss 0.00002]\n",
    "[epoch   85/100] [iter  131500] [loss 0.00036]\n",
    "[epoch   85/100] [iter  132000] [loss 0.00583]\n",
    "[epoch   86/100] [iter  132500] [loss 0.00005]\n",
    "[epoch   86/100] [iter  133000] [loss 0.00024]\n",
    "[epoch   86/100] [iter  133500] [loss 0.00013]\n",
    "[Test on environment] [epoch 86/100] [score 254.30]\n",
    "[epoch   87/100] [iter  134000] [loss 0.00023]\n",
    "[epoch   87/100] [iter  134500] [loss 0.02134]\n",
    "[epoch   87/100] [iter  135000] [loss 0.00137]\n",
    "[epoch   87/100] [iter  135500] [loss 0.00203]\n",
    "[epoch   88/100] [iter  136000] [loss 0.00390]\n",
    "[epoch   88/100] [iter  136500] [loss 0.00000]\n",
    "[epoch   88/100] [iter  137000] [loss 0.00000]\n",
    "[Test on environment] [epoch 88/100] [score 262.10]\n",
    "[epoch   89/100] [iter  137500] [loss 0.00001]\n",
    "[epoch   89/100] [iter  138000] [loss 0.00085]\n",
    "[epoch   89/100] [iter  138500] [loss 0.00002]\n",
    "[epoch   90/100] [iter  139000] [loss 0.00519]\n",
    "[epoch   90/100] [iter  139500] [loss 0.00361]\n",
    "[epoch   90/100] [iter  140000] [loss 0.02290]\n",
    "[Test on environment] [epoch 90/100] [score 262.50]\n",
    "[epoch   91/100] [iter  140500] [loss 0.00071]\n",
    "[epoch   91/100] [iter  141000] [loss 0.00083]\n",
    "[epoch   91/100] [iter  141500] [loss 0.02687]\n",
    "[epoch   92/100] [iter  142000] [loss 0.00005]\n",
    "[epoch   92/100] [iter  142500] [loss 0.00004]\n",
    "[epoch   92/100] [iter  143000] [loss 0.00038]\n",
    "[Test on environment] [epoch 92/100] [score 288.00]\n",
    "[epoch   93/100] [iter  143500] [loss 0.00711]\n",
    "[epoch   93/100] [iter  144000] [loss 0.00116]\n",
    "[epoch   93/100] [iter  144500] [loss 0.01607]\n",
    "[epoch   94/100] [iter  145000] [loss 0.00002]\n",
    "[epoch   94/100] [iter  145500] [loss 0.00000]\n",
    "[epoch   94/100] [iter  146000] [loss 0.00003]\n",
    "[Test on environment] [epoch 94/100] [score 264.90]\n",
    "[epoch   95/100] [iter  146500] [loss 0.00017]\n",
    "[epoch   95/100] [iter  147000] [loss 0.00000]\n",
    "[epoch   95/100] [iter  147500] [loss 0.00001]\n",
    "[epoch   95/100] [iter  148000] [loss 0.00045]\n",
    "[epoch   96/100] [iter  148500] [loss 0.00000]\n",
    "[epoch   96/100] [iter  149000] [loss 0.00828]\n",
    "[epoch   96/100] [iter  149500] [loss 0.03963]\n",
    "[Test on environment] [epoch 96/100] [score 255.40]\n",
    "[epoch   97/100] [iter  150000] [loss 0.00000]\n",
    "[epoch   97/100] [iter  150500] [loss 0.00000]\n",
    "[epoch   97/100] [iter  151000] [loss 0.00040]\n",
    "[epoch   98/100] [iter  151500] [loss 0.00932]\n",
    "[epoch   98/100] [iter  152000] [loss 0.00006]\n",
    "[epoch   98/100] [iter  152500] [loss 0.00003]\n",
    "[Test on environment] [epoch 98/100] [score 274.30]\n",
    "[epoch   99/100] [iter  153000] [loss 0.02656]\n",
    "[epoch   99/100] [iter  153500] [loss 0.00005]\n",
    "[epoch   99/100] [iter  154000] [loss 0.02065]\n",
    "[epoch  100/100] [iter  154500] [loss 0.00000]\n",
    "[epoch  100/100] [iter  155000] [loss 0.00060]\n",
    "[epoch  100/100] [iter  155500] [loss 0.00354]\n",
    "[Test on environment] [epoch 100/100] [score 268.30]\n",
    "Saving model as behavioral_cloning_CartPole-v1.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 pts]** Did you manage to learn a good policy? How consistent is the reward you are getting?\n",
    "\n",
    "The learned policy is better than the tabular Q learning and faster as well. The average reward is around 250. It has a 40ish variance. Min=210, max= 299 on policy evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [24 pts] Deep Q Learning\n",
    "\n",
    "There are two main issues with the behavior cloning approach.\n",
    "\n",
    "- First, we are not always lucky enough to have access to a dataset of expert demonstrations.\n",
    "- Second, replicating an expert policy suffers from compounding error. The policy $\\pi$ only sees these \"perfect\" examples and has no knowledge on how to recover from states not visited by the expert. For this reason, as soon as it is presented with a state that is off the expert trajectory, it will perform poorly and will continue to deviate from a good trajectory without the possibility of recovering from errors.\n",
    "\n",
    "---\n",
    "The second task consists in solving the environment from scratch, using RL, and most specifically the DQN algorithm, to learn a policy $\\pi$.\n",
    "\n",
    "For this task, familiarize yourself with the file `dqn.py`. We are going to re-use the file `model.py` for the model you created in the previous task.\n",
    "\n",
    "Your task is very similar to the one in the previous assignment, to implement the Q-learning algorithm, but in this version, our Q-function is approximated with a neural network.\n",
    "\n",
    "The algorithm (excerpted from [Atari DQN paper](https://arxiv.org/abs/1312.5602)) is given below:\n",
    "\n",
    "![DQN algorithm](https://i.imgur.com/Mh4Uxta.png)\n",
    "\n",
    "### 2.0 [2 pts] Think about your model...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DQN, we are using the same model as in task 1 for behavioral cloning. In both tasks the model receives as input the state and in both tasks the model outputs something that has the same dimensionality as the number of actions. These two outputs, though, represent very different things. What is each one representing?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 [10 pts] Update your Q-function\n",
    "\n",
    "Complete the `optimize_model` function. This function receives as input a `state`, an `action`, the `next_state`, the `reward` and `done` representing the tuple $(s_t, a_t, s_{t+1}, r_t, done_t)$. Your task is to update your Q-function as shown in the [Atari DQN paper](https://arxiv.org/abs/1312.5602) environment. For now don't be concerned with the experience replay buffer. We'll get to that later.\n",
    "\n",
    "![Loss function](https://i.imgur.com/tpTsV8m.png)\n",
    "\n",
    "Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR optimize_model function here:\n",
    "\n",
    "# def optimize_model(state, action, next_state, reward, done):\n",
    "#     # TODO given a tuple (s_t, a_t, s_{t+1}, r_t, done_t) update your model weights\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 [5 pts] $\\epsilon$-greedy strategy\n",
    "\n",
    "You will need a strategy to explore your environment. The standard strategy is to use $\\epsilon$-greedy. Implement it in the `choose_action` function template.\n",
    "\n",
    "Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR choose_action function here:\n",
    "\n",
    "# def choose_action(state, test_mode=False):\n",
    "#     # TODO implement an epsilon-greedy strategy\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 [2 pts] Train your model\n",
    "\n",
    "Try to train a model in this way.\n",
    "\n",
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 dqn.py\n",
    "```\n",
    "\n",
    "How many episodes does it take to learn (ie. reach a good reward)?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 [5 pts] Add the Experience Replay Buffer\n",
    "\n",
    "If you read the DQN paper (and as you can see from the algorithm picture above), the authors make use of an experience replay buffer to learn faster. We provide an implementation in the file `replay_buffer.py`. Update the `train_reinforcement_learning` code to push a tuple to the replay buffer and to sample a batch for the `optimize_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the replay buffer improve performance?\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extra (fully optional)\n",
    "\n",
    "Ideas to experiment with:\n",
    "\n",
    "- Is $\\epsilon$-greedy strategy the best strategy available? Experiment with other strategies.\n",
    "- Make use of the model you have trained in the behavioral cloning part and fine-tune it with RL. How does that affect performance?\n",
    "- You are perhaps bored with `CartPole-v1` by now. Another environment we suggest trying is `LunarLander-v2`. It will be harder to learn but with experimentation, you will find the correct optimizations for success. Piazza is also your friend :)\n",
    "- What about learning from images? This requires more work because you have to extract the image from the environment. How much more challenging might you expect the learning to be in this case?\n",
    "- An improvement over DQN is DoubleDQN. Experiment with this to see how much of an impact it makes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN USE THIS CODEBLOCK AND ADD ANY BLOCK BELOW AS YOU NEED\n",
    "# TO SHOW US THE IDEAS AND EXTRA EXPERIMENTS YOU RUN.\n",
    "# HAVE FUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
